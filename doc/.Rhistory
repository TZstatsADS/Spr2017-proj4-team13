order(table(originalID),decreasing = TRUE)
names(sort(table(AKumar$AuthorID)
,decreasing = TRUE))
table(AKumar$AuthorID)
AKumar$AuthorID
originalID
class(originalID)
as.numeric(orginalID)
as.numeric(originalID)
order(tableas.numeric(originalID))
order(table(as.numeric(originalID)))
order(table(originalID))
originalID <- as.numeric(mapvalues(AKumar$AuthorID,names(sort(table(AKumar$AuthorID),decreasing = TRUE)),c(1:14)))
estimateID <- mapvalues(group.14,order(table(group.14),decreasing = TRUE),c(1:14))
order(table(originalID))
order(table(estimateID))
table(originalID)
sum(originalID==estimateID)
n
a<-0
b<-0
c<-0
d<-0
for(i in 1:n){
for(j in c(1:n)[-i]){
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]==group.14[j]) a<-a+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]==group.14[j]) b<-b+1
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]!=group.14[j]) c<-c+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]!=group.14[j]) d<-d+1
}
}
a
d
n*(n-1)/2
a<-0
b<-0
c<-0
d<-0
for(i in 1:n){
for(j in (i+1):n){
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]==group.14[j]) a<-a+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]==group.14[j]) b<-b+1
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]!=group.14[j]) c<-c+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]!=group.14[j]) d<-d+1
}
}
a<-0
b<-0
c<-0
d<-0
for(i in 1:(n-1)){
for(j in (i+1):n){
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]==group.14[j]) a<-a+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]==group.14[j]) b<-b+1
if(AKumar$AuthorID[i]==AKumar$AuthorID[j]&group.14[i]!=group.14[j]) c<-c+1
if(AKumar$AuthorID[i]!=AKumar$AuthorID[j]&group.14[i]!=group.14[j]) d<-d+1
}
}
n*(n-1)/2
a+b+c+d
a/(a+b)
a+d
a+d/(a+b+c+d)
(a+d)/(a+b+c+d)
precision <- a/(a+b)
recall <- a/(a+c)
2*precition*recall/(precision+recall)
2*precision*recall/(precision+recall)
recall
precision
matching_matrix <- function(G,M){
n <- length(G)
result_matrix <- matrix(rep(0,4),ncol=2,nrow=2)
for(i in 1:(n-1)){
for(j in (i+1):n){
if(G[i]==G[j]&M[i]==M[j]) result_matrix[1,1]<-result_matrix[1,1]+1
if(G[i]!=G[j]&M[i]==M[j]) result_matrix[1,2]<-result_matrix[1,2]+1
if(G[i]==G[j]&M[i]!=M[j]) result_matrix[2,1]<-result_matrix[2,1]+1
if(G[i]!=G[j]&M[i]!=M[j]) result_matrix[2,2]<-result_matrix[2,2]+1
}
}
return(result_matrix)
}
result_matrix <- matching_matrix(AKumar$AuthorID,group.14)
result_matrix
(761+20383)/n
n
nrow(dtm_train)
(761+20383)/(n*(n-1)/2)
sum(result_matrix)
n*(n-1)/2
length(unique(AKumar$AuthorID))
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
table(result_hclust)
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_statistics <- function(result_matrix){
precision <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[1,2])
recall <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[2,1])
f1 <- 2*precision*recall/(precision+recall)
accuracy <- (result_matrix[1,1]+result_matrix[2,2])/sum(result_matrix)
}
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
performance_statistics <- function(result_matrix){
precision <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[1,2])
recall <- result_matrix[1,1]/(result_matrix[1,1]+result_matrix[2,1])
f1 <- 2*precision*recall/(precision+recall)
accuracy <- (result_matrix[1,1]+result_matrix[2,2])/sum(result_matrix)
return(list(precision=precision, recall=recall, f1=f1, accuracy=accuracy))
}
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
setwd("./Project4_WhoIsWho")
setwd("~/Dropbox/Project4_WhoIsWho")
setwd("~/Dropbox/Project4_WhoIsWho")
AKumar <- data.frame(scan("~/Dropbox/Project4_WhoIsWho/data/nameset/AKumar.txt",
what = list(Coauthor = "", Paper = "", Journal = ""), sep=">", quiet=TRUE),stringsAsFactors=FALSE)
source('~/Dropbox/Project4_WhoIsWho/lib/evaluation_measures.R')
vocab <- create_vocabulary(it_train)
vocab
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
n <- nrow(dtm_train)
cmb <- expand.grid(i=1:n, j=1:n)
docsdissim <- as.dist(matrix(apply(cmb,1,cos.sim),n,n))
rownames(docsdissim) <- c(1:n)
colnames(docsdissim) <- c(1:n)
h <- hclust(docsdissim, method = "ward.D")
n
dim(docsdissim)
h <- hclust(docsdissim, method = "ward.D")
plot(h, labels = c(1:n), sub = "")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
length(unique(AKumar$AuthorID))
unique(AKumar$AuthorID)
setwd("~/Dropbox/Project4_WhoIsWho/doc")
AKumar <- data.frame(scan("~/Dropbox/Project4_WhoIsWho/data/nameset/AKumar.txt",
what = list(Coauthor = "", Paper = "", Journal = ""),
sep=">", quiet=TRUE),stringsAsFactors=FALSE)
# This need to be modified for different name set
# extract canonical author id befor "_"
AKumar$AuthorID <- sub("_.*","",AKumar$Coauthor)
# extract paper number under same author between "_" and first whitespace
AKumar$PaperNO <- sub(".*_(\\w*)\\s.*", "\\1", AKumar$Coauthor)
# delete "<" in AKumar$Coauthor, you may need to further process the coauthor
# term depending on the method you are using
AKumar$Coauthor <- gsub("<","",sub("^.*?\\s","", AKumar$Coauthor))
# delete "<" in AKumar$Paper
AKumar$Paper <- gsub("<","",AKumar$Paper)
# add PaperID for furthur use, you may want to combine all the nameset files and
# then assign the unique ID for all the citations
AKumar$PaperID <- rownames(AKumar)
it_train <- itoken(AKumar$Paper,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = AKumar$PaperID,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab <- create_vocabulary(it_train)
vocab
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
cos.sim <- function(ix)
{
A = dtm_train[ix[1],]
B = dtm_train[ix[2],]
return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}
n <- nrow(dtm_train)
cmb <- expand.grid(i=1:n, j=1:n)
docsdissim <- as.dist(matrix(apply(cmb,1,cos.sim),n,n))
rownames(docsdissim) <- c(1:n)
h <- hclust(docsdissim, method = "ward.D")
plot(h, labels = c(1:n), sub = "")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
table(result_hclust)
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
vocab <- create_vocabulary(it_train, stopwords = c("a", "the", "in", "on", "at", "above", "under"))
vocab
vocab <- prune_vocabulary(vocab,
term_count_min = 1
doc_proportion_max = 0.95,
doc_proportion_min = 0.001)
vocab <- prune_vocabulary(vocab,
term_count_min = 1,
doc_proportion_max = 0.95,
doc_proportion_min = 0.001)
vocab
26+14+61+15+100+16+12+31+10+13+13+12+86+71
577+244+800+368+1417+112+171+927+280+153+259+412+1458+1264
dtm_train_tfidf[1:10,1:10]
dtm_train_tfidf[10:20,10:20]
dtm_train_tfidf[1:20,10:20]
dtm_train[1:20,10:20]
cos.sim <- function(ix)
{
A = dtm_train_tfidf[ix[1],]
B = dtm_train_tfidf[ix[2],]
return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}
n <- nrow(dtm_train)
cmb <- expand.grid(i=1:n, j=1:n)
docsdissim <- as.dist(matrix(apply(cmb,1,cos.sim),n,n))
h <- hclust(docsdissim, method = "ward.D")
plot(h, labels = c(1:n), sub = "")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
table(result_hclust)
source('~/Dropbox/Project4_WhoIsWho/lib/evaluation_measures.R')
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
View(AKumar)
my.data
data(UScitiesD)
mds2 <- -cmdscale(UScitiesD)
plot(mds2, type="n", axes=FALSE, ann=FALSE)
text(mds2, labels=rownames(mds2), xpd = NA)
UScitiesD
hcity.D  <- hclust(UScitiesD, "ward.D") # "wrong"
hcity.D2 <- hclust(UScitiesD, "ward.D2")
opar <- par(mfrow = c(1, 2))
plot(hcity.D,  hang=-1)
plot(hcity.D2, hang=-1)
require(graphics)
USArrests
dist(USArrests)
dist(USArrests)[1,1]
class(dist(USArrests))
?dist
as.matrix(dist(USArrests))[1,2]
USArrests[1,]
USArrests[2,]
dist(USArrests[1,],USArrests[2,])
USArrests[1,]%*%USArrests[2,]
t(USArrests[1,])%*%USArrests[2,]
t(as.vector(USArrests[1,]))%*%as.vector(USArrests[2,])
class(USArrests[2,])
as.vector(USArrests[2,])
USArrests[1,]
10*13.2+263*236+48*58+44.5*21.2
sqrt(65927.4)
as.matrix(dist(USArrests))[1,1]
3.2^2+27^2+100+(44.5-21.2)^2
sqrt(1382.13)
pacman::p_load(text2vec, dplyr, qlcMatrix)
dim(cosSparse(dtm_train_tfidf))
n
dim(dtm_train_tfidf)
class(dtm_train_tfidf)
dim(cosSparse(t(dtm_train_tfidf)))
nrow(dtm_train_tfidf)
docsdissim <- cosSparse(t(dtm_train_tfidf))
rownames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
colnames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
#compute pairwise cosine similarities between citations, using cosSparse function in package
h <- hclust(as.dist(docsdissim), method = "ward")
h <- hclust(as.dist(docsdissim), method = "ward.D")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
table(result_hclust)
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
performance_hclust
result_hclust <- specc(dtm_train_tfidf, centers=length(unique(AKumar$AuthorID)))
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab)
result_hclust <- specc(dtm_train_tfidf, centers=length(unique(AKumar$AuthorID)))
result_hclust <- specc(as.matrix(dtm_train_tfidf), centers=length(unique(AKumar$AuthorID)))
```
result_hclust <- specc(as.matrix(dtm_train_tfidf), centers=length(unique(AKumar$AuthorID)))
result_sclust <- specc(as.matrix(dtm_train_tfidf), centers=length(unique(AKumar$AuthorID)))
table(result_sclust)
matching_matrix_sclust <- matching_matrix(AKumar$AuthorID,result_sclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
performance_sclust
performance_hclust
?table
start.time <- Sys.time()
result_sclust <- specc(as.matrix(dtm_train_tfidf), centers=length(unique(AKumar$AuthorID)))
end.time <- Sys.time()
time_sclust <- end.time - start.time
time_sclust
start.time <- Sys.time()
docsdissim <- cosSparse(t(dtm_train_tfidf))
rownames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
colnames(docsdissim) <- c(1:nrow(dtm_train_tfidf))
#compute pairwise cosine similarities between citations, using cosSparse function in package
h <- hclust(as.dist(docsdissim), method = "ward.D")
result_hclust <- cutree(h,length(unique(AKumar$AuthorID)))
end.time <- Sys.time()
time_hclust <- end.time - start.time
time_hclust
performance_sclust
source('~/Dropbox/Project4_WhoIsWho/lib/evaluation_measures.R')
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
matching_matrix_sclust <- matching_matrix(AKumar$AuthorID,result_sclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
compare_df <- data.frame(method=c(sClust,hClust),
precision=c(performance_sclust$precision, performance_hclust$precision),
recall=c(performance_sclust$recall, performance_hclust$recall),
f1=c(performance_sclust$f1, performance_hclust$f1),
accuracy=c(performance_sclust$accuracy, performance_hclust$accuracy),
time=c(time_sclust,time_hclust))
kable(compare_df,caption="Comparision of performance for two clustering methods",digits = 2)
pacman::p_load(text2vec, dplyr, qlcMatrix, kernlab, knitr)
matching_matrix_hclust <- matching_matrix(AKumar$AuthorID,result_hclust)
performance_hclust <- performance_statistics(matching_matrix_hclust)
matching_matrix_sclust <- matching_matrix(AKumar$AuthorID,result_sclust)
performance_sclust <- performance_statistics(matching_matrix_sclust)
compare_df <- data.frame(method=c("sClust","hClust"),
precision=c(performance_sclust$precision, performance_hclust$precision),
recall=c(performance_sclust$recall, performance_hclust$recall),
f1=c(performance_sclust$f1, performance_hclust$f1),
accuracy=c(performance_sclust$accuracy, performance_hclust$accuracy),
time=c(time_sclust,time_hclust))
kable(compare_df,caption="Comparision of performance for two clustering methods",digits = 2)
end.time - start.time
A = matrix(1:6,ncol=3)
Q = qr(A)
Q
B = qr(A)
qr.decom = qr(A)
Q = qr.Q(qr.decom)
Q
R = qr.R(qr.decom)
R
q%*%R
Q%*%R
A
print(t(R)%*%R)
(t(R)%*%R)[order(qr.decom$pivot), order(qr.decom$pivot)]
b = matrix(c(20, 33, 10, 12, 14, 22, 34, 55, 11, 40, 0, 0, 0, 0, 0, 33,40, 66,
78, 90, 11, 45, 32, 55, 65), nrow = 5, ncol= 5)
A =crossprod(b)
A
qr.decom = qr(-b, 1e-20)
qr.decom
R = qr.R(qr.decom)
R
Q = qr.Q(qr.decom)
Q
Q%*%R
-b
print(t(R)%*%R)
?crossprod
A
A = matrix(rep(1,9),nrow=3)
A
qr.decom = qr(A,LAPACK = TRUE)
qr.decom
Q = qr.Q(qr.decom)
R = qr.R(qr.decom)
X = qr.X(qr.decom)
X
Q
R
Q%*%R
A <- matrix(c(12, 6, -4, -51, 167, 24, 4, -68, -41), nrow=3)
Bqr = qr(A, LAPACK=TRUE)
Bqr
Q = qr.Q(Bqr)
P = qr.P(Bqr)
R = qr.R(Bqr)
Q%*%R
qr.X(Bqr)
?qr
I = diag(nrow = Nrow)
Nrow =5
I = diag(nrow = Nrow)
I
column.order = c(1,3,2,4,5)
Y.P = I[,column.order]
Y,p
Y.p
Y.P
R_hat = Y.R %*% t(Y.P)
x=c(1,2,3,4,9,1,1,1)
which.max(x)
x=c(1,2,3,4,9,1,1,10)
which.max(x)
library(mlbench)
set.seed(111)
obj = mlbench.spirals(100,1,0.025)
my.dat =  4 * obj$x
head(my.dat)
sigma = 1
n.cluster=2
Nrow = nrow(my.dat)
Nrow
Affinity = matrix(rep(0,Nrow^2), ncol=Nrow)
dim(Affinity)
for(i in 1 : Nrow){
for(j in 1 : Nrow){
if(i != j){
Affinity[i,j] = exp(-(1/2*(sigma^2)) * norm(as.matrix(my.dat[i,]-my.dat[j,]), type="F")^2)}
}
}
diag(Affinity)
Affinity[1:5,1:5]
Degree = diag(apply(Affinity,1,sum))
Degree
Degree = diag(apply(Affinity,1,sum))
diag(Degree)
Degree[1:5,1:5]
Lap = (Degree %^% (-1/2)) %*% Affinity %*% (Degree %^% (-1/2))
library(expm)
Lap = (Degree %^% (-1/2)) %*% Affinity %*% (Degree %^% (-1/2))
ev = eigen(Lap, symmetric = TRUE)#Calculate the eigenvectors of Lap
ev$values
ev$vectors
n.cluster
X = ev$vectors[,1:n.cluster]
X
Y = t(X)
Y
Y.decom = qr(Y,LAPACK = TRUE)
Y.R = qr.R(Y.decom)
column.order = Y.decom$pivot
column.order
I = diag(nrow = Nrow)
I
I = diag(nrow = Nrow)
Y.P = I[,column.order]
apply(I,2,which.max) == column.order
apply(I,2,which.max)
Y.P[1:2,1:2]
column.order
apply(Y.P,2,which.max) == column.order
R_hat = Y.R %*% t(Y.P)
cluster = apply(R_hat,2,which.max)
table(cluster)
length(cluster)
cluster
plot = data.frame(x=my.dat[,1],y=my.dat[,2],class = cluster)
plot
plot = data.frame(x=my.dat[,1],y=my.dat[,2],class = cluster)
par(mfrow=c(1,2))
plot(my.dat,main="Original")
plot(x = plot$x, y = plot$y, col=plot$cluster,main="results")
par(mfrow=c(1,2))
plot(my.dat,main="Original")
plot(x = plot$x, y = plot$y, col=plot$class,main="results")
library(stats)
K = kmeans(my.dat, n.cluster=2,nstart = 5)
?kmeans
K = kmeans(my.dat, centers=2,nstart = 5)
library(stats)
K = kmeans(my.dat, centers=2,nstart = 5)
par(mfrow=c(1,2))
plot(x = plot$x, y = plot$y, col=plot$class,main="Spectral Cluster with QR")
plot(my.dat, col=k$cluster,main="K means")
plot(my.dat, col=K$cluster,main="K means")
library(stats)
K = kmeans(my.dat, centers=2,nstart = 5)
par(mfrow=c(1,2))
plot(x = plot$x, y = plot$y, col=plot$class,main="Spectral Cluster with QR")
plot(my.dat, col=K$cluster,main="K means")
library(mlbench)
set.seed(111)
obj = mlbench.spirals(100,1,0.025)
my.dat =  4 * obj$x
#This is a function that performs the spectral cluster algorithm
#Package needed: stats, expm
#Input:
#my.dat: data matrix
#sigma: numerical values indicating bandwidth of RBF kernel
#n.cluster: numerical value indeicates the number of clusters
#output:
#km.ouput: A k-means object that contains clluster results
library(stats)
library(expm)
Spectral.Cluster = function(my.dat,sigma = 1, n.cluster=2) {
Nrow = nrow(my.dat)
#Construct Affinity matrix
Affinity = matrix(rep(0,Nrow^2), ncol=Nrow)
for(i in 1 : Nrow){
for(j in 1 : Nrow){
if(i != j){
Affinity[i,j] = exp(-(1/2*(sigma^2)) * norm(as.matrix(my.dat[i,]-my.dat[j,]), type="F")^2)}
}
}
#COnstruct degree matrix Degree
Degree = diag(apply(Affinity,1,sum))
#Construct normalizaed graph Laplacian
Lap = (Degree %^% (-1/2)) %*% Affinity %*% (Degree %^% (-1/2))
ev = eigen(Lap, symmetric = TRUE)#Calculate the eigenvectors of Lap
#Form matrix X by staking the n.cluster largest eigenvectors from Lap
X = ev$vectors[,1:n.cluster]
#My parts ended
##########################################################################################################
#Final   = ev$vectors[,1:n.cluster]#Locate the n.cluster largest eigenvectors to construct final matrix
Final   = ev$vectors[,(ncol(ev$vectors)-n.cluster+1):ncol(ev$vectors)]#Locate the n.cluster smallest eigenvectors to construct final matrix
#perform k-means algorithm to cluster the final matrix
km.ouput = kmeans(Final, centers=n.cluster, nstart=5)
return(km.ouput)
}
A = Spectral.Cluster(my.dat,n.line=3,n.cluster=2,normal = F)
B = Spectral.Cluster(my.dat,n.line=3,n.cluster=2,normal = T)
par(mfrow=c(1,2))
plot(my.dat,main="Original")
plot(my.dat, col=A$cluster,main="Unnormalized Lap")
par(mfrow=c(1,2))
plot(my.dat,main="Original")
plot(my.dat, col=B$cluster,main="normalized Lap")
